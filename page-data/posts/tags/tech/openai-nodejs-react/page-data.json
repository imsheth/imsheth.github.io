{"componentChunkName":"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js","path":"/posts/tags/tech/openai-nodejs-react","result":{"data":{"mdx":{"id":"33e6eeb0-b322-5874-8571-42fda1dff97d","excerpt":"Oct 17, 2024 This post is focused on using node.js 19.6.0 and react to integrate with openai 4.68.0 package on macOS Ventura 13.7 OpenAI 4.68.0 Using   ChatGPTâ€¦","fields":{"slug":"/posts/tags/tech/openai-nodejs-react/"},"frontmatter":{"title":"The Hows - OpenAI APIs with node.js and react on macOS Ventura 13.7","description":"This post is focused on using node.js and react to integrate with OpenAI APIs on macOS Ventura 13.7","image":"/posts/airflow2-dockeroperator-nodejs-gitlab/airflow2-dockeroperator-nodejs-gitlab-banner.png","disableTableOfContents":false},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"The Hows - OpenAI APIs with node.js and react on macOS Ventura 13.7\",\n  \"description\": \"This post is focused on using node.js and react to integrate with OpenAI APIs on macOS Ventura 13.7\",\n  \"image\": \"/posts/airflow2-dockeroperator-nodejs-gitlab/airflow2-dockeroperator-nodejs-gitlab-banner.png\",\n  \"disableTableOfContents\": false\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Oct 17, 2024\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    style: {\n      \"textAlign\": \"center\"\n    }\n  }, \"The motivation for writing this post is that it provides a single concise starting point to explore OpenAI with node.js and react.\")), mdx(\"p\", null, \"This post is focused on using node.js 19.6.0 and react to integrate with openai 4.68.0 package on macOS Ventura 13.7\"), mdx(\"a\", {\n    href: \"https://github.com/imsheth/openai-nodejs-react\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"The relevant source code for the post can be found here\"), mdx(\"br\", null), mdx(\"br\", null), mdx(\"hr\", null), mdx(\"h2\", {\n    \"id\": \"openai-4680\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#openai-4680\",\n    \"aria-label\": \"openai 4680 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"OpenAI 4.68.0\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://openai.com/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"OpenAI\"), \" is the organization that created \", mdx(\"a\", {\n    href: \"https://platform.openai.com/docs/models\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"generative AI models\"), \" and packaged it into a product called \", mdx(\"a\", {\n    href: \"https://chat.openai.com/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"ChatGPT\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Using \", mdx(\"a\", {\n    href: \"https://chat.openai.com/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \" ChatGPT \"), \" is free (ChatGPT Plus and ChatGPT Team are paid) but using \", mdx(\"a\", {\n    href: \"https://community.openai.com/t/is-playground-free/18909/14\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \" OpenAI playground \"), \" is not after you exhaust your free credits (if any)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We'll be using \", mdx(\"a\", {\n    href: \"https://www.npmjs.com/package/openai\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \" OpenAI Node API Library openai 4.68.0 \"))), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    href: \"https://platform.openai.com/docs/overview\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Sign up for an OpenAI account\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    href: \"https://platform.openai.com/settings/organization/billing/overview\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Add payment method and add credits to balance\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    href: \"https://platform.openai.com/api-keys\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Create new secret key and copy it\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"For the \\\"Owned by\\\"\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"b\", null, \"You\"), \" is better for personal use and if you're going to be the only person making API calls. This API key is tied to your user and can make requests against the selected project. If you are removed from the organization or project, this key will be disabled. The \\\"Permissions\\\" setting lets you adjust the permissions and granularity the key will have.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"b\", null, \"Service account\"), \" is for organizational use and adds more security layers, so it's suitable for programmatic API calls through the app/system you're building. This API key is not tied to your user and can make requests against the selected project. If you are removed from the organization or project, this key will not be disabled.\")))))), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/1.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/1.png\",\n    alt: \"openai-nodejs-react-1-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/2.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/2.png\",\n    alt: \"openai-nodejs-react-2-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/3.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/3.png\",\n    alt: \"openai-nodejs-react-3-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"h2\", {\n    \"id\": \"nodejs-1960\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#nodejs-1960\",\n    \"aria-label\": \"nodejs 1960 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"node.js 19.6.0\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Initialize project, install packages, create server file and assign environment variable for server port, api key\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"mkdir server && cd server && npm init -y && npm install --save express body-parser cors openai && touch index.js && export OPENAI_API_SERVER_PORT=8080 && export OPENAI_API_KEY=\\\"<your_api_key_here>\\\"\\n\")), mdx(\"ol\", {\n    \"start\": 2\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Add the following line to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"package.json\"), \" \", mdx(\"a\", {\n    href: \"https://nodejs.org/api/packages.html#determining-module-system\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \" to make the server file an ES module\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"\\\"type\\\": \\\"module\\\",\\n\")), mdx(\"ol\", {\n    \"start\": 3\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Below \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"node.js\"), \" code exposes endpoints that in-turn call OpenAI APIs and \", mdx(\"a\", {\n    href: \"https://github.com/imsheth/openai-nodejs-react/tree/main/postman-collection\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \" the relevant postman collection v2.1 can be found here\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-javascript\"\n  }, \"// Import packages\\nimport express from \\\"express\\\";\\nimport bodyParser from \\\"body-parser\\\";\\nimport OpenAI from \\\"openai\\\";\\nimport cors from \\\"cors\\\";\\nimport fs from \\\"fs\\\";\\nimport path from \\\"path\\\";\\n\\n// Initialize\\nconst app = express();\\nconst port = process.env[\\\"OPENAI_API_SERVER_PORT\\\"];\\nconst openai = new OpenAI();\\nconst generatedSpeechFile = path.resolve(\\\"./generated_speech.mp3\\\");\\nlet threadRunStatusInterval;\\n\\n// Parse incoming requests with JSON payloads\\n// Body parser is a middleware for Node.js that parses incoming request bodies and makes them available as objects in the req.body property\\n// https://medium.com/@amirakhaled2027/body-parser-middleware-in-node-js-75b6b9a36613\\napp.use(bodyParser.json());\\n\\n// Allow CORS\\napp.use(cors());\\n\\n// Setup server on specified port\\napp.listen(port, () => {\\n  console.log(`Server listening on port ${port}`);\\n});\\n\\n// POST /text_generation/chat_completions that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/chat/create\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/text_generation/chat_completions\\\", async (request, response) => {\\n  const { chats } = request.body;\\n\\n  const result = await openai.chat.completions.create({\\n    model: \\\"gpt-3.5-turbo\\\",\\n    messages: [\\n      { role: \\\"system\\\", content: \\\"You are a texas cowboy.\\\" },\\n      ...chats,\\n    ],\\n  });\\n\\n  response.json({\\n    output: result.choices[0].message,\\n  });\\n});\\n\\n// POST /image_generation/images that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/images/create\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/image_generation/images\\\", async (request, response) => {\\n  const generatedImage = await openai.images.generate({\\n    model: \\\"dall-e-3\\\",\\n    prompt: request.body.prompt_message,\\n    size: \\\"1024x1024\\\",\\n    quality: \\\"standard\\\",\\n    n: 1,\\n  });\\n\\n  response.json({\\n    output: {\\n      imageURL: generatedImage.data[0].url,\\n    },\\n  });\\n});\\n\\n// // POST /vision/images that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/chat/create\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/vision/images\\\", async (request, response) => {\\n  const visionComprehension = await openai.chat.completions.create({\\n    model: \\\"gpt-4o-mini\\\",\\n    messages: [\\n      {\\n        role: \\\"user\\\",\\n        content: [\\n          { type: \\\"text\\\", text: \\\"What's in this image?\\\" },\\n          {\\n            type: \\\"image_url\\\",\\n            image_url: {\\n              url: request.body.image_url,\\n            },\\n          },\\n        ],\\n      },\\n    ],\\n  });\\n  response.json({\\n    output: {\\n      imageURL: request.body.image_url,\\n      imageComprehension: visionComprehension.choices[0],\\n    },\\n  });\\n});\\n\\n// POST /text_to_speech/audio_speeches that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/audio/createSpeech\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\n// Supported languages https://platform.openai.com/docs/guides/text-to-speech/supported-languages\\n// You can generate spoken audio in these languages by providing the input text in the language of your choice.\\n// Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan,\\n// Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician,\\n// German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada,\\n// Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian,\\n// Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili,\\n// Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh\\n// Thought not specified, it also supports Gujarati\\napp.post(\\\"/text_to_speech/audio_speeches\\\", async (request, response) => {\\n  const generatedSpeech = await openai.audio.speech.create({\\n    model: \\\"tts-1\\\",\\n    response_format: \\\"mp3\\\",\\n    voice: \\\"onyx\\\",\\n    input: request.body.prompt_message,\\n    speed: \\\"1\\\",\\n  });\\n\\n  const buffer = Buffer.from(await generatedSpeech.arrayBuffer());\\n  await fs.promises.writeFile(generatedSpeechFile, buffer);\\n\\n  console.log(\\\"generatedSpeechFile \\\", generatedSpeechFile);\\n\\n  response.json({\\n    output: {\\n      speechFileGeneration: \\\"successful\\\",\\n    },\\n  });\\n});\\n\\n// POST /speech_to_text/audio_transcrptions that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/audio/createTranscription\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\n// Thought not specified, it doesn't support Gujarati and lacks accuracy for Hindi\\napp.post(\\\"/speech_to_text/audio_transcrptions\\\", async (request, response) => {\\n  const speechTranscription = await openai.audio.transcriptions.create({\\n    file: fs.createReadStream(generatedSpeechFile),\\n    model: \\\"whisper-1\\\",\\n    response_format: \\\"json\\\",\\n  });\\n\\n  console.log(\\\"speechTranscription \\\", speechTranscription);\\n\\n  response.json({\\n    output: {\\n      speechTranscript: speechTranscription.text,\\n    },\\n  });\\n});\\n\\n// POST /speech_to_text/audio_translations that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/audio/createTranslation\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/speech_to_text/audio_translations\\\", async (request, response) => {\\n  const speechTranslation = await openai.audio.translations.create({\\n    file: fs.createReadStream(generatedSpeechFile),\\n    model: \\\"whisper-1\\\",\\n    response_format: \\\"json\\\",\\n  });\\n\\n  console.log(\\\"speechTranslation \\\", speechTranslation);\\n\\n  response.json({\\n    output: {\\n      speechTranslated: speechTranslation.text,\\n    },\\n  });\\n});\\n\\n// POST /vector_embeddings/embeddings that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/embeddings/create\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/vector_embeddings/embeddings\\\", async (request, response) => {\\n  const embedding = await openai.embeddings.create({\\n    model: \\\"text-embedding-3-small\\\",\\n    input: request.body.input_text.replace(/[\\\\n\\\\r]/g, \\\" \\\"),\\n    encoding_format: \\\"float\\\",\\n    dimensions: 512,\\n  });\\n\\n  console.log(\\\"embedding \\\", embedding);\\n\\n  response.json({\\n    output: {\\n      embeddedVector: embedding.data[0].embedding,\\n    },\\n  });\\n});\\n\\n// POST /moderation/moderations that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/moderations\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\n// Text moderation works only for English\\napp.post(\\\"/moderation/moderations\\\", async (request, response) => {\\n  let inputContent = [];\\n  inputContent.push({ type: \\\"text\\\", text: request.body.input_text });\\n  if (request.body.input_url) {\\n    inputContent.push({\\n      type: \\\"image_url\\\",\\n      image_url: {\\n        url: request.body.input_url,\\n      },\\n    });\\n  }\\n  console.log(\\\"inputContent \\\", inputContent);\\n  const moderation = await openai.moderations.create({\\n    model: \\\"omni-moderation-latest\\\",\\n    input: inputContent,\\n  });\\n\\n  response.json({\\n    output: {\\n      moderationAnalysis: moderation.results,\\n    },\\n  });\\n});\\n\\n// POST /reasoning/chat_completions that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/chat\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\n// Text moderation works for English and not Hindi/Gujarati\\napp.post(\\\"/reasoning/chat_completions\\\", async (request, response) => {\\n  const completion = await openai.chat.completions.create({\\n    model: \\\"o1-preview\\\",\\n    messages: [\\n      {\\n        role: \\\"user\\\",\\n        content: request.body.input_text.trim(),\\n      },\\n    ],\\n  });\\n\\n  response.json({\\n    output: completion.choices[0].message,\\n  });\\n});\\n\\n// POST /assistant/assistants that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/assistants/createAssistant\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/assistant/assistants\\\", async (request, response) => {\\n  const assistant = await openai.beta.assistants.create({\\n    name: request.body.input_assistant_name,\\n    description: request.body.input_assistant_desc,\\n    model: \\\"gpt-4o\\\",\\n    // tools: [{ type: \\\"code_interpreter\\\" }],\\n  });\\n\\n  response.json({\\n    output: assistant,\\n  });\\n});\\n\\n// POST /assistant/threads that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/threads\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/assistant/threads\\\", async (request, response) => {\\n  const thread = await openai.beta.threads.create();\\n\\n  response.json({\\n    output: thread,\\n  });\\n});\\n\\nasync function checkThreadRunStatus(res, threadId, runId) {\\n  const threadRunStatusResponse = await openai.beta.threads.runs.retrieve(\\n    threadId,\\n    runId\\n  );\\n  console.log(\\n    \\\"threadRunStatusResponse.status \\\" + threadRunStatusResponse.status\\n  );\\n\\n  if (threadRunStatusResponse.status == \\\"completed\\\") {\\n    clearInterval(threadRunStatusInterval);\\n\\n    const threadMessages = await openai.beta.threads.messages.list(threadId);\\n    let messages = [];\\n\\n    threadMessages.body.data.forEach((message) => {\\n      messages.push(message.content);\\n    });\\n\\n    res.json({ messages });\\n  }\\n}\\n\\n// POST /assistant/messages that calls OpenAI API\\n// API reference https://platform.openai.com/docs/api-reference/messages\\n// API library https://github.com/openai/openai-node/blob/master/api.md\\napp.post(\\\"/assistant/messages\\\", async (request, response) => {\\n  const message = await openai.beta.threads.messages.create(\\n    request.body.thread_id,\\n    {\\n      role: \\\"user\\\", // \\\"user\\\" or \\\"assistant\\\" only\\n      content: request.body.input_message,\\n    }\\n  );\\n\\n  const threadRun = await openai.beta.threads.runs.create(message.thread_id, {\\n    assistant_id: request.body.assistant_id,\\n  });\\n\\n  // Check status and return response if run is completed\\n  threadRunStatusInterval = setInterval(() => {\\n    checkThreadRunStatus(response, message.thread_id, threadRun.id);\\n  }, 2500);\\n});\\n\")), mdx(\"ol\", {\n    \"start\": 4\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Run the server and test the endpoints\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"node -v && node index.js\\n\")), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/4.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/4.png\",\n    alt: \"openai-nodejs-react-4-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/5.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/5.png\",\n    alt: \"openai-nodejs-react-5-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/6.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/6.png\",\n    alt: \"openai-nodejs-react-6-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/7.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/7.png\",\n    alt: \"openai-nodejs-react-7-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/8.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/8.png\",\n    alt: \"openai-nodejs-react-8-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/9.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/9.png\",\n    alt: \"openai-nodejs-react-9-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/10.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/10.png\",\n    alt: \"openai-nodejs-react-10-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/11.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/11.png\",\n    alt: \"openai-nodejs-react-11-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/12.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/12.png\",\n    alt: \"openai-nodejs-react-12-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/13.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/13.png\",\n    alt: \"openai-nodejs-react-13-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/14.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/14.png\",\n    alt: \"openai-nodejs-react-14-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/15.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/15.png\",\n    alt: \"openai-nodejs-react-15-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/16.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/16.png\",\n    alt: \"openai-nodejs-react-16-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/17.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/17.png\",\n    alt: \"openai-nodejs-react-17-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/18.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/18.png\",\n    alt: \"openai-nodejs-react-18-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/19.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/19.png\",\n    alt: \"openai-nodejs-react-19-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/20.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/20.png\",\n    alt: \"openai-nodejs-react-20-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"h2\", {\n    \"id\": \"react-app\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#react-app\",\n    \"aria-label\": \"react app permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"react-app\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Create a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"react-app\"), \" using \", mdx(\"a\", {\n    href: \"https://create-react-app.dev/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"create-react-app \"), \" and local environment is ready at \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"http://localhost:3000/\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-shell\"\n  }, \"npx create-react-app react-app && cd react-app && export OPENAI_API_SERVER_PORT=8080 && npm start\\n\")), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/21.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/21.png\",\n    alt: \"openai-nodejs-react-21-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"ol\", {\n    \"start\": 2\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Below \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"app.js\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"react\"), \" code consumes the server endpoints that in-turn call OpenAI APIs\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-javascript\"\n  }, \"import \\\"./App.css\\\";\\nimport { useState } from \\\"react\\\";\\n\\nfunction App() {\\n  const [chatMessage, setChatMessage] = useState(\\\"\\\");\\n  const [chats, setChats] = useState([]);\\n  const [isTyping, setIsTyping] = useState(false);\\n\\n  const [imageDescription, setImageDescription] = useState(\\\"\\\");\\n  const [imageAnalysis, setImageAnalysis] = useState(\\\"\\\");\\n  const [imageUrl, setImageUrl] = useState(\\\"\\\");\\n\\n  const [audioDescription, setAudioDescription] = useState(\\\"\\\");\\n  const [audioGenerated, setAudioGenerated] = useState(false);\\n  const [audioTranscription, setAudioTranscription] = useState(\\\"\\\");\\n  const [audioTranslation, setAudioTranslation] = useState(\\\"\\\");\\n\\n  const [vectorEmbeddings, setVectorEmbeddings] = useState(\\\"\\\");\\n  const [generatedVectorEmbedding, setGeneratedVectorEmbedding] = useState(\\\"\\\");\\n\\n  const [moderationText, setModerationText] = useState(\\\"\\\");\\n  const [moderationURL, setModerationURL] = useState(\\\"\\\");\\n  const [moderationResult, setModerationResult] = useState(null);\\n\\n  const [assistantChatMessage, setAssistantChatMessage] = useState(\\\"\\\");\\n  const [assistantChats, setAssistantChats] = useState([]);\\n\\n  const assistantChat = async (e, inputText) => {\\n    e.preventDefault();\\n\\n    try {\\n      const request1 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          assistant_id: \\\"<Your assistant id here>\\\",\\n          thread_id: \\\"<Your assistant id here>\\\",\\n          input_message: inputText,\\n        }),\\n      };\\n\\n      const assistantResponse = await fetch(\\n        `http://localhost:8080/assistant/messages`,\\n        request1\\n      );\\n      const data1 = await assistantResponse.json();\\n\\n      if (data1.messages) {\\n        setAssistantChats(\\n          data1.messages\\n            .flat()\\n            .reverse()\\n            .map((i) => i.text.value)\\n        );\\n      }\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  const performModeration = async (e, inputText, inputURL) => {\\n    e.preventDefault();\\n\\n    try {\\n      const request1 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          input_text: inputText,\\n          input_url: inputURL,\\n        }),\\n      };\\n\\n      const moderationAnalysis = await fetch(\\n        `http://localhost:8080/moderation/moderations`,\\n        request1\\n      );\\n      const data1 = await moderationAnalysis.json();\\n\\n      if (data1.output.moderationAnalysis) {\\n        setModerationResult(data1.output.moderationAnalysis);\\n      }\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  const generateEmbedding = async (e, inputText) => {\\n    e.preventDefault();\\n\\n    try {\\n      const request1 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          input_text: inputText,\\n        }),\\n      };\\n\\n      const generatedEmbedding = await fetch(\\n        `http://localhost:8080/vector_embeddings/embeddings`,\\n        request1\\n      );\\n      const data1 = await generatedEmbedding.json();\\n\\n      if (data1.output.embeddedVector) {\\n        setGeneratedVectorEmbedding(data1.output.embeddedVector.join(\\\", \\\"));\\n      }\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  const generateAndAnalyzeAudio = async (e, inputText) => {\\n    e.preventDefault();\\n\\n    try {\\n      const request1 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          prompt_message: inputText,\\n        }),\\n      };\\n\\n      const generatedAudio = await fetch(\\n        `http://localhost:8080/text_to_speech/audio_speeches`,\\n        request1\\n      );\\n      const data1 = await generatedAudio.json();\\n\\n      if (data1.output.speechFileGeneration === \\\"successful\\\") {\\n        setAudioGenerated(true);\\n\\n        const request2 = {\\n          method: \\\"POST\\\",\\n          headers: {\\n            \\\"Content-Type\\\": \\\"application/json\\\",\\n          },\\n          body: \\\"\\\",\\n        };\\n\\n        const generatedAudioTranscription = await fetch(\\n          `http://localhost:8080/speech_to_text/audio_transcrptions`,\\n          request2\\n        );\\n        const data2 = await generatedAudioTranscription.json();\\n        if (data2.output.speechTranscript) {\\n          setAudioTranscription(data2.output.speechTranscript);\\n        }\\n\\n        const request3 = {\\n          method: \\\"POST\\\",\\n          headers: {\\n            \\\"Content-Type\\\": \\\"application/json\\\",\\n          },\\n          body: \\\"\\\",\\n        };\\n\\n        const generatedAudioTranslation = await fetch(\\n          `http://localhost:8080/speech_to_text/audio_translations`,\\n          request3\\n        );\\n        const data3 = await generatedAudioTranslation.json();\\n        if (data3.output.speechTranslated) {\\n          setAudioTranslation(data3.output.speechTranslated);\\n        }\\n      }\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  const generateAndAnalyzeImage = async (e, inputText) => {\\n    e.preventDefault();\\n\\n    try {\\n      const request1 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          prompt_message: inputText,\\n        }),\\n      };\\n\\n      const generatedImage = await fetch(\\n        `http://localhost:8080/image_generation/images`,\\n        request1\\n      );\\n      const data1 = await generatedImage.json();\\n\\n      const request2 = {\\n        method: \\\"POST\\\",\\n        headers: {\\n          \\\"Content-Type\\\": \\\"application/json\\\",\\n        },\\n        body: JSON.stringify({\\n          image_url: data1.output.imageURL,\\n        }),\\n      };\\n\\n      const generatedImageAnalysis = await fetch(\\n        `http://localhost:8080/vision/images`,\\n        request2\\n      );\\n      const data2 = await generatedImageAnalysis.json();\\n\\n      setImageAnalysis(data2.output.imageComprehension.message.content);\\n      setImageUrl(data1.output.imageURL);\\n      setImageDescription(\\\"\\\");\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  const completeChat = async (e, chatMessage) => {\\n    e.preventDefault();\\n\\n    if (!chatMessage) return;\\n\\n    setIsTyping(true);\\n    let msgs = chats;\\n    msgs.push({ role: \\\"user\\\", content: chatMessage });\\n    setChats(msgs);\\n\\n    setChatMessage(\\\"\\\");\\n\\n    const request = {\\n      method: \\\"POST\\\",\\n      headers: {\\n        \\\"Content-Type\\\": \\\"application/json\\\",\\n      },\\n      body: JSON.stringify({\\n        chats,\\n      }),\\n    };\\n    try {\\n      const fetchResponse = await fetch(\\n        `http://localhost:8080/text_generation/chat_completions`,\\n        request\\n      );\\n      const data = await fetchResponse.json();\\n      msgs.push(data.output);\\n      setChats(msgs);\\n      setIsTyping(false);\\n    } catch (error) {\\n      console.log(error);\\n    }\\n  };\\n\\n  return (\\n    <main>\\n      <h1>OpenAI node.js React</h1>\\n      <hr />\\n\\n      <section>\\n        <h2>/text_generation/chat_completions</h2>\\n        {chats && chats.length\\n          ? chats.map((chat, index) => (\\n              <p key={index} className={chat.role === \\\"user\\\" ? \\\"user_msg\\\" : \\\"\\\"}>\\n                <span>\\n                  {chat.role === \\\"user\\\" ? <b>User : </b> : <b>OpenAI : </b>}\\n                </span>\\n                <span>{chat.content}</span>\\n              </p>\\n            ))\\n          : \\\"\\\"}\\n\\n        <div className={isTyping ? \\\"\\\" : \\\"hide\\\"}>\\n          <p>\\n            <i>{isTyping ? \\\"Typing\\\" : \\\"\\\"}</i>\\n          </p>\\n        </div>\\n\\n        <form action=\\\"\\\" onSubmit={(e) => completeChat(e, chatMessage)}>\\n          <input\\n            type=\\\"text\\\"\\n            name=\\\"chatMessage\\\"\\n            value={chatMessage}\\n            placeholder=\\\"Chat message\\\"\\n            onChange={(e) => setChatMessage(e.target.value)}\\n          />\\n        </form>\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/image_generation/images & /vision/images</h2>\\n\\n        <input\\n          type=\\\"text\\\"\\n          name=\\\"imageDescriptionMessage\\\"\\n          value={imageDescription}\\n          placeholder=\\\"Prompt message to generate and analyze that image\\\"\\n          onChange={(e) => setImageDescription(e.target.value)}\\n        />\\n        <button\\n          style={{ width: \\\"100%\\\" }}\\n          onClick={(e) => generateAndAnalyzeImage(e, imageDescription)}\\n        >\\n          Generate image and analyze\\n        </button>\\n        {imageAnalysis && (\\n          <p>\\n            <span>{imageAnalysis}</span>\\n          </p>\\n        )}\\n        {imageUrl && <img src={imageUrl} alt=\\\"Loaded\\\" />}\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>\\n          /text_to_speech/audio_speeches, /speech_to_text/audio_transcrptions &\\n          /speech_to_text/audio_translations\\n        </h2>\\n\\n        <input\\n          type=\\\"text\\\"\\n          name=\\\"audioDescriptionMessage\\\"\\n          value={audioDescription}\\n          placeholder=\\\"Prompt message to generate and transcribe + translate that audio\\\"\\n          onChange={(e) => setAudioDescription(e.target.value)}\\n        />\\n        <button\\n          style={{ width: \\\"100%\\\" }}\\n          onClick={(e) => generateAndAnalyzeAudio(e, audioDescription)}\\n        >\\n          Generate audio and transcribe + translate\\n        </button>\\n        {audioGenerated && (\\n          <p>\\n            <span>Audio generated successfully</span>\\n          </p>\\n        )}\\n        {audioTranscription && audioTranscription.length > 0 && (\\n          <p>\\n            <span>Audio transcription - {audioTranscription}</span>\\n          </p>\\n        )}\\n        {audioTranslation && audioTranslation.length > 0 && (\\n          <p>\\n            <span>Audio translation - {audioTranslation}</span>\\n          </p>\\n        )}\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/vector_embeddings/embeddings</h2>\\n\\n        <input\\n          type=\\\"text\\\"\\n          name=\\\"vectorText\\\"\\n          value={vectorEmbeddings}\\n          placeholder=\\\"Text to generate vector embeddings\\\"\\n          onChange={(e) => setVectorEmbeddings(e.target.value)}\\n        />\\n        <button\\n          style={{ width: \\\"100%\\\" }}\\n          onClick={(e) => generateEmbedding(e, vectorEmbeddings)}\\n        >\\n          Generate generate vector embeddings\\n        </button>\\n        {generatedVectorEmbedding && generatedVectorEmbedding.length > 0 && (\\n          <p>\\n            <span>Generated vector embedding - {generatedVectorEmbedding}</span>\\n          </p>\\n        )}\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/moderation/moderations</h2>\\n\\n        <input\\n          type=\\\"text\\\"\\n          name=\\\"moderationText\\\"\\n          value={moderationText}\\n          placeholder=\\\"Text to moderate\\\"\\n          onChange={(e) => setModerationText(e.target.value)}\\n        />\\n        <input\\n          type=\\\"text\\\"\\n          name=\\\"moderationURL\\\"\\n          value={moderationURL}\\n          placeholder=\\\"URL to moderate\\\"\\n          onChange={(e) => setModerationURL(e.target.value)}\\n        />\\n        <button\\n          style={{ width: \\\"100%\\\" }}\\n          onClick={(e) => performModeration(e, moderationText, moderationURL)}\\n        >\\n          Generate moderation analysis\\n        </button>\\n        {moderationResult && moderationResult.length > 0 && (\\n          <p>\\n            <span>\\n              Generated moderation - content to be flagged -{\\\" \\\"}\\n              {moderationResult[0].flagged.toString()}\\n            </span>\\n            <br />\\n            <br />\\n            <span>Generated moderation - content analysis categories -</span>\\n            <br />\\n            <br />\\n            {Object.entries(moderationResult[0].categories).map(\\n              ([key, value]) => (\\n                <div>{`${key}: ${value}`}</div>\\n              )\\n            )}\\n            <span>Generated moderation - content analysis scores -</span>\\n            <br />\\n            <br />\\n            {Object.entries(moderationResult[0].category_scores).map(\\n              ([key, value]) => (\\n                <div>{`${key}: ${value}`}</div>\\n              )\\n            )}\\n          </p>\\n        )}\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/reasoning/chat_completions</h2>\\n        <p>\\n          <span>\\n            o1 models are currently in beta with limited features and access to\\n            developers in certain usage tiers only\\n          </span>\\n        </p>\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/assistant/assistants, /assistant/threads</h2>\\n        <p>\\n          <span>\\n            Use postman to create assistant and thread to avoid duplications\\n          </span>\\n        </p>\\n      </section>\\n\\n      <hr />\\n\\n      <section>\\n        <h2>/assistant/messages</h2>\\n\\n        {assistantChats && assistantChats.length\\n          ? assistantChats.map((chat, index) => (\\n              <p key={index} className={index % 2 ? \\\"user_msg\\\" : \\\"\\\"}>\\n                <span>{index % 2 ? <b>User : </b> : <b>OpenAI : </b>}</span>\\n                <span>{chat}</span>\\n              </p>\\n            ))\\n          : \\\"\\\"}\\n\\n        {/* <div className={isTyping ? \\\"\\\" : \\\"hide\\\"}>\\n          <p>\\n            <i>{isTyping ? \\\"Typing\\\" : \\\"\\\"}</i>\\n          </p>\\n        </div> */}\\n\\n        <form\\n          action=\\\"\\\"\\n          onSubmit={(e) => assistantChat(e, assistantChatMessage)}\\n        >\\n          <input\\n            type=\\\"text\\\"\\n            name=\\\"assistantChatMessage\\\"\\n            value={assistantChatMessage}\\n            placeholder=\\\"Chat message with existing thread and assistant\\\"\\n            onChange={(e) => setAssistantChatMessage(e.target.value)}\\n          />\\n        </form>\\n      </section>\\n    </main>\\n  );\\n}\\n\\nexport default App;\\n\")), mdx(\"ol\", {\n    \"start\": 3\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Below \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"index.js\"), \" code makes the above \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"app.js\"), \" usable\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-css\"\n  }, \":root {\\n  font-family: -apple-system, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Roboto\\\",\\n    \\\"Oxygen\\\", \\\"Ubuntu\\\", \\\"Cantarell\\\", \\\"Fira Sans\\\", \\\"Droid Sans\\\",\\n    \\\"Helvetica Neue\\\", sans-serif;\\n  -webkit-font-smoothing: antialiased;\\n  -moz-osx-font-smoothing: grayscale;\\n}\\n\\nmain {\\n  max-width: 1024px;\\n  margin: auto;\\n}\\n\\nh1,\\nh2 {\\n  text-align: center;\\n}\\n\\np {\\n  background-color: lightskyblue;\\n  padding: 10px;\\n}\\n\\n.user_msg {\\n  text-align: right;\\n}\\n\\n.hide {\\n  visibility: hidden;\\n  display: none;\\n}\\n\\ninput {\\n  width: 98%;\\n  border: 1px solid #000000;\\n  padding: 10px;\\n  font-size: 1.1rem;\\n}\\n\\ninput:focus {\\n  outline: none;\\n}\\n\")), mdx(\"ol\", {\n    \"start\": 4\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Test the endpoints from frontend \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"react\"), \" app at \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"http://localhost:3000/\"))), mdx(\"a\", {\n    href: \"/posts/openai-nodejs-react/22.png\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, mdx(\"img\", {\n    src: \"/posts/openai-nodejs-react/22.png\",\n    alt: \"openai-nodejs-react-22-png\",\n    align: \"left\",\n    style: {\n      \"marginRight\": \"2rem\",\n      \"marginBottom\": \"1rem\"\n    }\n  })), mdx(\"h2\", {\n    \"id\": \"references\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#references\",\n    \"aria-label\": \"references permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"References\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://medium.com/leniolabs/exploring-openais-apis-assistants-vs-chat-completions-91525f73422c\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for OpenAI chap completion and assistant APIs\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://www.freecodecamp.org/news/how-to-build-a-chatbot-with-openai-chatgpt-nodejs-and-react/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for text generation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://medium.com/@kylehe970128/how-to-use-openai-api-in-react-js-enhancing-your-applications-with-ai-in-2024-02e248fdc889\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for image generation and vision\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://medium.com/@sathishhariram/openai-embedding-semantic-search-using-vector-data-b785ae7079ff\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for vector embedding generation, store and search 1\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://markus.oberlehner.net/blog/your-own-vector-search-in-5-minutes-with-sqlite-openai-embeddings-and-nodejs/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for vector embedding generation, store and search 2\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://dev.to/momentohq/how-to-build-a-question-answering-system-in-nodejs-with-a-vector-index-and-openai-3dkd\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for vector embedding generation, store and search 3 (with chunking)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    href: \"https://serpapi.com/blog/assistant-api-openai-beginner-tutorial/\",\n    target: \"_blank\",\n    rel: \"noopener noreferrer\"\n  }, \"Reference for assistant API\"))), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#openai\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#openai4.68.0\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#nodejs\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#openai19.6.0\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#react\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#create-react-app\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"#tech\")), mdx(\"hr\", null), mdx(\"div\", {\n    \"style\": {\n      \"textAlign\": \"left\",\n      \"marginTop\": \"0.55rem\",\n      \"marginBottom\": \"0.40rem\"\n    }\n  }, \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://twitter.com/theimsheth\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/twitter.svg\",\n    \"alt\": \"twitter\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://github.com/imsheth\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/github.svg\",\n    \"alt\": \"github\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://www.npmjs.com/~imsheth\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/npm.svg\",\n    \"alt\": \"npm\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://stackoverflow.com/users/3152654/imsheth?tab=profile\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/stackoverflow.svg\",\n    \"alt\": \"stackoverflow\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://in.linkedin.com/in/imsheth\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/linkedin.svg\",\n    \"alt\": \"linkedin\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://instagram.com/imsheth/\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/instagram.svg\",\n    \"alt\": \"instagram\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://www.facebook.com/imsheth\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/facebook.svg\",\n    \"alt\": \"facebook\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://open.spotify.com/user/orekqqr23uij1ehi7upg9akat?si=xXA0ZUqxQW2GgFYzdXgAOA&nd=1\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/spotify.svg\",\n    \"alt\": \"spotify\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \"), \"\\n  \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://www.tvshowtime.com/1589595\",\n    \"target\": \"_blank\",\n    \"rel\": \"noopener noreferrer\",\n    \"style\": {\n      \"textDecoration\": \"none\"\n    }\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"/social/tvtime.svg\",\n    \"alt\": \"tvshowtime\",\n    \"style\": {\n      \"height\": \"32px\",\n      \"width\": \"32px\",\n      \"margin\": \".2rem\"\n    }\n  }), \"\\n  \")), mdx(\"hr\", null), mdx(\"br\", null), mdx(Giscus, {\n    id: \"comments\",\n    repo: \"imsheth/imsheth.github.io\",\n    repoId: \"MDEwOlJlcG9zaXRvcnk2Mjg4MDczMw==\",\n    category: \"Announcements\",\n    categoryId: \"DIC_kwDOA7973c4CP60t\",\n    mapping: \"url\",\n    term: \"Welcome to @giscus/react component!\",\n    reactionsEnabled: \"1\",\n    emitMetadata: \"1\",\n    inputPosition: \"top\",\n    theme: \"light\",\n    lang: \"en\",\n    loading: \"lazy\",\n    mdxType: \"Giscus\"\n  }));\n}\n;\nMDXContent.isMDXComponent = true;","headings":[{"depth":2,"value":"OpenAI 4.68.0"},{"depth":2,"value":"node.js 19.6.0"},{"depth":2,"value":"react-app"},{"depth":2,"value":"References"}]}},"pageContext":{"slug":"/posts/tags/tech/openai-nodejs-react/","next":{"label":"Meta","link":"/"},"repositoryEditUrl":"https://github.com/imsheth/imsheth.github.io/tree/main/src/docs/posts/tags/tech/openai-nodejs-react.mdx","repositoryProvider":"GitHub"}},"staticQueryHashes":["1954253342","2328931024","2501019404","973074209"]}